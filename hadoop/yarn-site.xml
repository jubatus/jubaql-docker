<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <property>
      <name>yarn.application.classpath</name>
      <value>/usr/local/hadoop/etc/hadoop, /usr/local/hadoop/share/hadoop/common/*, /usr/local/hadoop/share/hadoop/common/lib/*, /usr/local/hadoop/share/hadoop/hdfs/*, /usr/local/hadoop/share/hadoop/hdfs/lib/*, /usr/local/hadoop/share/hadoop/mapreduce/*, /usr/local/hadoop/share/hadoop/mapreduce/lib/*, /usr/local/hadoop/share/hadoop/yarn/*, /usr/local/hadoop/share/hadoop/yarn/lib/*</value>
    </property>

    <property>
    <description>
      Number of seconds after an application finishes before the nodemanager's
      DeletionService will delete the application's localized file directory
      and log directory.

      To diagnose Yarn application problems, set this property's value large
      enough (for example, to 600 = 10 minutes) to permit examination of these
      directories. After changing the property's value, you must restart the
      nodemanager in order for it to have an effect.

      The roots of Yarn applications' work directories is configurable with
      the yarn.nodemanager.local-dirs property (see below), and the roots
      of the Yarn applications' log directories is configurable with the
      yarn.nodemanager.log-dirs property (see also below).
    </description>
    <name>yarn.nodemanager.delete.debug-delay-sec</name>
    <value>600</value>
  </property>

  <!-- Above this line is the default configuration from
       https://github.com/sequenceiq/hadoop-docker/blob/2.6.0/yarn-site.xml -->

  <!-- When there is not enough memory available, submitted applications
       will stay forever in ACCEPTED state and never change to RUNNING;
       in particular there is no error thrown. So we increase the available
       memory, decrease the minimum memory required for a container, and
       disable the checks for physical or virtual memory limits. -->

  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>7000</value>
  </property>

  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>256</value>
  </property>

  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
    <!-- Also see
    https://github.com/cloudera/oryx/wiki/FAQ-and-Troubleshooting -->
  </property>

  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
  </property>

  <!-- A similar situation seems to exist if there are not enough VCPUs
       available, so we set the number to something that is high enough
       for this demo. -->

  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>16</value>
  </property>

  <!-- We need to be able to retrieve logs from HDFS. -->

  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>

</configuration>
